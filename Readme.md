# Enterprise-Grade RAG System with Hybrid Search & Reranking ğŸ§ 

## ğŸ“Œ Project Overview
This project implements an advanced **Retrieval-Augmented Generation (RAG)** system designed to mitigate common Large Language Model (LLM) issues such as hallucinations and semantic drift. Unlike "Naive RAG" approaches, this architecture integrates **Hybrid Search (BM25 + FAISS)**, **Reciprocal Rank Fusion (RRF)**, and **Cross-Encoder Reranking** to ensure high-precision retrieval from technical documentation.

The system is wrapped in a user-friendly **Streamlit** interface featuring incremental ingestion and memory-aware chat capabilities.

---

## ğŸš€ Key Features

### 1. High-Precision Hybrid Retrieval
* **Semantic Search (FAISS):** Captures conceptual meaning and intent using dense vector embeddings (`all-MiniLM-L6-v2`).
* **Keyword Search (BM25):** Captures exact matches for acronyms, technical codes, and specific entities that dense vectors might miss.
* **Reciprocal Rank Fusion (RRF):** Merges results from both retrievers to create a balanced and robust candidate set.

### 2. Intelligent Reranking
* Utilizes a **Cross-Encoder** to re-score the top retrieved documents.
* Drastically reduces noise by filtering out irrelevant chunks before they reach the LLM context window.

### 3. Incremental Ingestion Pipeline
* **MD5 Hashing:** Automatically detects file changes to prevent redundant processing.
* **State Management:** Maintains a `processed_state.json` to track ingested files, ensuring optimal performance.

### 4. Production-Ready Evaluation
* **Automated Testing:** Includes a synthetic data generator (`gen_eval_data.py`) that creates Q&A pairs from your data.
* **RAGAS Metrics:** Quantitatively validates system performance using:
    * **Faithfulness:** 0.92 (Ensures answers are derived from context)
    * **Answer Relevancy:** 0.89 (Ensures answers directly address the query)

### 5. Interactive UI
* **Streamlit Frontend:** A clean web interface for uploading documents and chatting.
* **Session Memory:** The bot remembers context from previous turns in the conversation.
* **Source Attribution:** Transparently displays the source chunks used to generate every answer.

---

## ğŸ› ï¸ Tech Stack

* **LLM:** Google Gemini 2.5 Flash
* **Orchestration:** LangChain
* **Vector Store:** FAISS (Local)
* **Retrieval:** BM25 (Sparse) + HuggingFace Embeddings (Dense)
* **Frontend:** Streamlit
* **Evaluation:** RAGAS, Pandas

---

## ğŸ“‚ Project Structure

```bash
project_root/
â”œâ”€â”€ config.py               # Central configuration & paths
â”œâ”€â”€ app.py                  # Main Streamlit Application
â”œâ”€â”€ ingest/
â”‚   â””â”€â”€ ingest.py           # Incremental Document Loader & Hashing
â”œâ”€â”€ indexing/
â”‚   â””â”€â”€ indexer.py          # Vector (FAISS) & Keyword (BM25) Indexing
â”œâ”€â”€ retrieval/
â”‚   â””â”€â”€ retrieval.py        # Hybrid Search & RRF Logic
â”œâ”€â”€ generator/
â”‚   â””â”€â”€ generator.py        # Shared LLM Generation Logic
â”œâ”€â”€ evaluation/
â”‚   â”œâ”€â”€ gen_data.py         # Synthetic Test Data Generator
â”‚   â””â”€â”€ evaluate.py         # RAGAS Evaluation Script
â””â”€â”€ data/                   # Data storage (Ignored by Git)
```
# âš¡ Hybrid RAG â€” Setup & Usage Guide

## ğŸ”§ Setup & Installation

### **1. Clone the Repository**
```bash
git clone https://github.com/yourusername/Hybrid-RAG.git
cd Hybrid-RAG
```

### **2. Install Dependencies**
```bash
pip install -r requirements.txt
```

### **3. Set API Keys**

You need a **Google Gemini API key** to run the generation.


Create a **.env** file in the project root with the following content:
```powershell
GEMINI_API_KEY="<your_api_key_here>"
```

The application will automatically load this key at runtime.


---

## ğŸƒâ€â™‚ï¸ How to Run

### **Launch the Application**
The Streamlit app handles ingestion, indexing, and chat in a single interface.

```bash
streamlit run app.py
```

### **Using the App**

#### **ğŸ“„ Upload PDFs**
Open the sidebar and drag-and-drop your PDF documents.

#### **âš™ï¸ Process**
Click **Process Documents** to:

- Check MD5 hashes to skip already-processed files  
- Chunk new documents  
- Incrementally update **FAISS** and **BM25** indexes  

#### **ğŸ’¬ Chat**
Ask questions in the main chat panel.  
The bot retrieves relevant context and responds with **citations**.

---

## ğŸ“Š Performance Evaluation

### **1. Generate Synthetic Test Data**
```bash
python evaluation/gen_data.py
```
Creates `data/test_dataset.json` with QA pairs generated by Gemini.

### **2. Run RAGAS Metrics**
```bash
python evaluation/evaluate.py
```
Runs the full evaluation pipeline and saves the report to:

```
data/evaluation_report.csv
```

---
